\chapter{Related Work}
\label{ch:Related Work}
The unique attributes and individualistic patterns inherent in brain signals have attracted considerable research interest towards constructing brainwave authentication systems. Consequently, this section will delve into a selection of pertinent studies that align closely with the research objectives of our study. We aim to explore and discuss the findings of these prior works, which have sought to address similar challenges that our research intends to tackle. 
%A large portion of these studies has centered around collecting EEG data from a selection of individuals and applying a singular authentication algorithm to distinguish between authentic and imposter users. This is primarily why a substantial amount of brainwave authentication research is predicated on the use of a single algorithm. The subsequent sections delve into the studies previously conducted on brainwave authentication, focusing on those based on a single authentication algorithm and a single dataset. Furthermore, research involving benchmarking with multiple authentication algorithms is also explored.

\section{Benchmarking works on Brianwave Authentication}
\label{sec:Related Work:Benchmarking works on Brianwave Authentication}
In brainwave authentication, benchmarking studies play a vital role in setting new standards and evaluating the effectiveness of newly proposed methods compared to the SOA authentication algorithms. This section overviews some of the benchmarking studies conducted for brainwave authentication. As noted in section \ref{sec:Background:Authentication Algorithms}, algorithms like SVM, LDA, and KNN have been widely adopted in many brainwave authentication studies such as \cite{kaur2017novel, jayarathne2016brainid, zuquete2010biometric} because of their simplicity and ability to find the discriminant patterns in brain signals. Jayarathne \textit{et al.} \cite{jayarathne2020person} in 2020 compared the algorithms as mentioned earlier performance on the EEG data recorded from 12 subjects using an EMOTIV Epoc+ headset. Accuracy was calculated for the different combinations of electrodes, and the best-performing classifier turned out to be KNN with $99.0 \pm 0.8\%$, followed by SVM achieving $98.03 \pm 0.1\%$ accuracy and LDA with $98.01 \pm 0.5\%$ accuracy respectively. Meanwhile, Huang \textit{et al.} \cite{huang2019eeg} opted for a different approach, concentrating on alternative algorithms like NB, NN, and LR. The study extracted seven statistical features from the data, such as mean, median, standard deviation, entropy, maximum, minimum, and skewness, to get a comprehensive insight into the data distribution, central tendency, and variation. 
Furthermore, performance metrics like ACC, TPR, FPR, and ROC-Curve were chosen to demonstrate to evaluate the performance of classifiers on the EEG data of 30 subjects. NB demonstrated the worst performance among the classifiers, registering average ACC, TPR, and FPR of 77.96$\%$, 75.71$\%$, and 19.80$\%$ respectively. LR has better performance than NB with average ACC, TPR, FPR of 81.59$\%$, 79.04$\%$, and 15.05$\%$ respectively. The NN classifier, however, displayed the most exceptional performance, recording average ACC, TPR, and FPR of 82.69$\%$, 81.96$\%$, and 17.38$\%$ respectively.
%Finally, the best performance was given by classifier NN with average ACC 82.69$\%$, TPR and FPR of 81.96$\%$ and 17.38$\%$ respectively. 

\section{Existing studies exploring cross-session variability}
\label{sec:Related Work:Existing studies exploring cross-session variability}
Although studies investigating the effects of intra-class variability across sessions in brainwave authentication are scarce, certain researchers have focused on this area. One of the most extensive works on this area was done by Huang \textit{et al.} \cite{huang2022m3cv}, in 2022, who had explored EEG variability across sessions, subjects, and tasks. The study contains EEG data from 106 subjects, 96 out of 106 participated in two sessions conducted on different days. Six paradigms, including resting state, transient state sensory, steady state sensory, cognitive oddball, motor execution, and steady-state sensory with selective attention, were conducted throughout the entire EEG experiment. 12th-order AR, PSD, and Mel Frequency Cepstral Coefficients (MFCCs) were chosen to extract the discriminant features from the brain signals, and the SVM classifier was employed to perform the identification and verification task. Additionally, Huang et al.'s research included both within-session and cross-session evaluations in the context of identification and verification. There was a noticeable performance decline in the cross-session evaluation compared to the within-session evaluation. In the verification task, the average EER across all paradigms increased twofold, escalating from 0.16 in within-session evaluation to 0.32 in cross-session evaluation. Similarly, in the identification task, the average accuracy fell dramatically from 0.70 in within-session to 0.31 in cross-session evaluation. This study's results show the necessity for greater research into EEG variability across sessions and subjects. 
\smallskip 

%Seha and Seha and Hatzinakos \cite{seha2020eeg} in 2020 presented their work on multi-session variability using steady-state Auditory Evoked Potentials (AEPs) for EEG-based recognition and the results of their study was far more better than Huang et al.'s results.

While the results of Huang et al.'s cross-session evaluation were inferior, Seha and Hatzinakos \cite{seha2020eeg} in 2020 produced impressive results in a similar study area using steady-state Auditory Evoked Potentials (AEPs) for EEG-based recognition. The study involved EEG experiment on 40 subjects across two sessions held on separate days. The study demonstrated exceptional results even when evaluated across cross-session (multiple-sessions). EER of mere 2-4$\%$ was achieved in cross-session evaluation that is 16 times more effective than that of Huang et al.'s work on cross-session evaluation. 

\section{Siamese Neural Networks in Brainwave Authentication Studies}
\label{sec:Related Work:Siamese Neural Networks in Brainwave Authentication Studies}
%\begin{itemize}
   As noted in section \ref{sec:Introduction:Problem Description}, most brainwave authentication studies employed SOA machine learning algorithms to discern between genuine users and imposters. These models often require the learning algorithms to retrain whenever new users are added to the system, which reduces the model's effectiveness and hinders practical application \cite{fallahi2023brainnet}. 
    Some studies proposed a solution to this problem by employing deep learning procedures to learn embeddings of the brain signals and subsequently calculating similarities between them. Following this approach, Bidgoly \textit{et al.} \cite{bidgoly2022towards} presented a notable study employing the publicly available Physionet dataset \cite{Phsionet} for brainwave authentication. The dataset contains EEG recordings from 109 subjects, captured as the subjects performed resting tasks for 5 seconds. The study utilized CNN to generate the brain embeddings during training and verify the authenticity of the new users by comparing their data with the stored samples using similarity metrics like Cosine Similarity, Euclidean Distance, and Manhattan Distance.
    %gauged the similarity between EEG samples using similarity metrics like . Their approach involved training the CNN model to generate the embeddings and verifying the authenticity of the new users by comparing their data with the stored samples. 
    The best-performing similarity function was Cosine Similarity with EER of just 1.96$\%$, followed by Manhattan and Euclidean with 3.91$\%$ and 5.65$\%$ EER, respectively. The study provides a more realistic scenario and addresses the key challenge of identifying new users whose brain data were not introduced during training. However, this approach may not be universally accepted since deep learning methods like CNN often require large amounts of data to optimize parameters during training, an aspect often impractical given the limited size of most brainwave datasets \cite{lotte2018review}.
    \smallskip
    
    Maiorana \cite{maiorana2019eeg} proposed a broader solution to overcome the problem of frequent retraining and to obtain the results with minimal EEG samples by employing the Siamese Neural Network approach. The study aimed to perform EEG-based verification and investigate the effects of intra-class variability across subjects whose brain signals were collected in 5 sessions over 15 months. Two identical CNNs received inputs in the form of the pre-processed brain samples and, then, were trained with the same parameters and weights to produce the brain embeddings. Afterward, the similarity of these embeddings was computed using Euclidean distance. The achieved EER was less than 7$\%$ for 30 seconds verification probe, a significantly good result considering the cross-session variability in brain data.
    \smallskip
    
   Lately Fallahi \textit{et al.} \cite{fallahi2023brainnet} presented their work on Siamese Networks for brainwave-based recognition in verification and identification mode. The study was conducted employing the EEG recordings from two publicly available EEG datasets such as \textbf{BrainInvaders (b12015a)} \cite{brainInvaders15a} and \textbf{ERP Core} \cite{erpcore}. Unlike Maiorana's \cite{maiorana2019eeg} methodology, which used contrastive loss function for determining the similar and dissimilar brain embeddings, Fallahi \textit{et al.} opted for a triplet loss function for their approach. As a result, three sub-networks, each with five convolution layers, produce embeddings, which were then evaluated under both close-set (i.e., seen attackers) and open-set (i.e., unseen attacker) scenarios. In verification mode, the calculated EERs for the close-set scenario were notably less than those of open-set scenarios, with dataset b12015a having an EER of a mere 0.14$\%$ for seen attackers. Similar trends were seen in identification mode where EER for the dataset b12015a was 0.34$\%$, the lowest among all the datasets.            
%\end{itemize}



